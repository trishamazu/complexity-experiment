{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get the embeddings for the 50 experiment images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data has been saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "df1 = pd.read_csv('/home/wallacelab/complexity-experiment/Data/Embeddings/hebart49_scaled_embedding_data.csv')  \n",
    "df2 = pd.read_csv('/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart_ranking_results.csv')  \n",
    "\n",
    "# Ensure 'Image Name' exists in both dataframes\n",
    "if 'Image Name' in df1.columns and 'Image Name' in df2.columns:\n",
    "    # Create sets of 'Image Name' from both dataframes for fast comparison\n",
    "    set_of_image_names_df2 = set(df2['Image Name'])\n",
    "\n",
    "    # Filter df1 to only include rows where 'Image Name' matches those in df2\n",
    "    filtered_df1 = df1[df1['Image Name'].isin(set_of_image_names_df2)]\n",
    "\n",
    "    # Save the filtered dataframe to a new CSV file\n",
    "    filtered_df1.to_csv('/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart49_embeddings.csv', index=False)\n",
    "    print(\"Filtered data has been saved successfully.\")\n",
    "else:\n",
    "    print(\"Error: 'Image Name' column missing in one or both datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use participant data to give each image a complexity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Image Name  Average Third Ordering  Normalized Average\n",
      "0     catapult_01b.jpg               16.666667            0.680272\n",
      "1         sled_01b.jpg               24.833333            0.513605\n",
      "2       alpaca_01b.jpg               27.833333            0.452381\n",
      "3       anchor_01b.jpg               25.833333            0.493197\n",
      "4  springboard_01b.jpg               22.833333            0.554422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart_ranking_results.csv')\n",
    "\n",
    "# Identify all columns that end with 'Third Ordering'\n",
    "third_ordering_cols = [col for col in df.columns if col.endswith('Third Ordering')]\n",
    "\n",
    "# Calculate the mean of these columns\n",
    "df['Average Third Ordering'] = df[third_ordering_cols].mean(axis=1)\n",
    "\n",
    "# Normalize the averages\n",
    "df['Normalized Average'] = 1 - ((df['Average Third Ordering'] - 1) / (50 - 1))\n",
    "\n",
    "# Select only the relevant columns\n",
    "final_df = df[['Image Name', 'Average Third Ordering', 'Normalized Average']]\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "final_df.to_csv('/home/wallacelab/complexity-experiment/Final Paper Data/ranking_complexity_scores.csv', index=False)\n",
    "print(final_df.head())  # Optionally print the first few rows to check the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now multiply the normalized scores by the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def sort_csv_by_image_name(input_csv_path, output_csv_path):\n",
    "    # Read the CSV file\n",
    "    with open(input_csv_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  \n",
    "        rows = list(reader)  \n",
    "\n",
    "    # Sort the rows\n",
    "    sorted_rows = sorted(rows, key=lambda x: x[0])\n",
    "\n",
    "    # Write the sorted rows back to a new CSV file\n",
    "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)  \n",
    "        writer.writerows(sorted_rows)  \n",
    "\n",
    "sort_csv_by_image_name('/home/wallacelab/complexity-experiment/Final Paper Data/ranking_complexity_scores.csv', '/home/wallacelab/complexity-experiment/Final Paper Data/ranking_complexity_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def multiply_values(average_csv_path, values_csv_path, output_csv_path):\n",
    "    # Read the CSV file with the 'Normalized Average'\n",
    "    with open(average_csv_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        averages = {row['Image Name']: float(row['Normalized Average']) for row in reader}\n",
    "\n",
    "    # Read the CSV file with the values, columns named \"0\" to \"48\"\n",
    "    with open(values_csv_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        fieldnames = reader.fieldnames\n",
    "        rows = [row for row in reader]\n",
    "\n",
    "    # Perform the multiplication for columns named \"0\" to \"48\"\n",
    "    multiplied_rows = []\n",
    "    for row in rows:\n",
    "        image_name = row['Image Name']\n",
    "        avg = averages.get(image_name, 1)  # Use average 1 if not found, or handle appropriately\n",
    "        # Multiply each specified value by the corresponding 'Normalized Average'\n",
    "        for i in range(49):  # Assuming column titles are strings \"0\" through \"48\"\n",
    "            if str(i) in row:\n",
    "                row[str(i)] = float(row[str(i)]) * avg\n",
    "        multiplied_rows.append(row)\n",
    "\n",
    "    # Write the results back to a new CSV file\n",
    "    with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(multiplied_rows)\n",
    "\n",
    "# Example usage\n",
    "multiply_values('/home/wallacelab/complexity-experiment/Final Paper Data/ranking_complexity_scores.csv', '/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart49_embeddings.csv', '/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart49_weighted_embeddings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add the embeddings together, turn them into a probability distribution, and determine which dimensions hold the most complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability distribution has been saved to /home/wallacelab/complexity-experiment/Final Paper Data/probability_distribution.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from a CSV file\n",
    "input_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart49_weighted_embeddings.csv'  # replace 'input.csv' with your file path\n",
    "data = pd.read_csv(input_csv)\n",
    "\n",
    "# List of column names to sum up\n",
    "columns_to_sum = [f'{i}' for i in range(49)]\n",
    "\n",
    "# Sum up the values in each specified column\n",
    "sums = data[columns_to_sum].sum()\n",
    "\n",
    "# Convert sums to a probability distribution\n",
    "total_sum = sums.sum()\n",
    "probability_distribution = sums / total_sum\n",
    "\n",
    "# Save the probability distribution to a new CSV file\n",
    "output_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/probability_distribution.csv'  # define the output CSV file name\n",
    "probability_distribution.to_csv(output_csv, header=True)\n",
    "\n",
    "print('Probability distribution has been saved to', output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked probabilities have been saved to /home/wallacelab/complexity-experiment/Final Paper Data/ranked_dimensions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the probability data from a CSV file\n",
    "input_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/probability_distribution.csv'  # replace 'probabilities.csv' with your file path\n",
    "data = pd.read_csv(input_csv)\n",
    "\n",
    "# Sort the data based on the 'Probability' column in descending order\n",
    "ranked_data = data.sort_values('Probability', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save the ranked data to a new CSV file\n",
    "output_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/ranked_dimensions.csv'  # define the output CSV file name\n",
    "ranked_data.to_csv(output_csv, index=False)\n",
    "\n",
    "print('Ranked probabilities have been saved to', output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now determine how to re-weight the entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights assigned and saved to /home/wallacelab/complexity-experiment/Final Paper Data/weighted_dimensions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from a CSV file\n",
    "input_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/probability_distribution.csv'  # replace 'probabilities.csv' with your file path\n",
    "data = pd.read_csv(input_csv)\n",
    "\n",
    "# Calculate the average probability\n",
    "average_probability = data['Probability'].mean()\n",
    "\n",
    "# Calculate the distance of each probability from the average\n",
    "data['Distance_from_average'] = abs(data['Probability'] - average_probability)\n",
    "\n",
    "# Find the maximum distance from the average\n",
    "max_distance = data['Distance_from_average'].max()\n",
    "\n",
    "# Assign weights based on the distance from the average\n",
    "data['Weight'] = data.apply(lambda row: 1 if row['Probability'] == average_probability + max_distance\n",
    "                            else (-1 if row['Probability'] == average_probability - max_distance\n",
    "                                  else ((row['Probability'] - average_probability) / max_distance)), axis=1)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "output_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/weighted_dimensions.csv'  # define the output CSV file name\n",
    "data.to_csv(output_csv, index=False)\n",
    "\n",
    "print('Weights assigned and saved to', output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine entropy and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column \"Entropy\" appended to /home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart49_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the existing data from the CSV file\n",
    "existing_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart49_embeddings.csv'  # replace 'existing_data.csv' with your file path\n",
    "existing_data = pd.read_csv(existing_csv)\n",
    "\n",
    "# Load the entropy data from another CSV file\n",
    "entropy_csv = '/home/wallacelab/complexity-experiment/Data/EntropyScores/things_entropy_scores.csv'  # replace 'entropy_data.csv' with your file path\n",
    "entropy_data = pd.read_csv(entropy_csv)\n",
    "\n",
    "# Merge the existing data with the entropy data based on 'Image Name'\n",
    "merged_data = pd.merge(existing_data, entropy_data, on='Image Name', how='left')\n",
    "\n",
    "# Save the updated data with the new column to the existing CSV file\n",
    "merged_data.to_csv(existing_csv, index=False)\n",
    "\n",
    "print('Column \"Entropy\" appended to', existing_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get the weights for the dimensions. The file just_weights.csv contains weights which can be used to reweight dimensional entropy scores for any image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed and saved to: /home/wallacelab/complexity-experiment/Final Paper Data/just_weights.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the input CSV file\n",
    "input_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/weighted_dimensions.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(input_csv)\n",
    "\n",
    "# Remove the first three columns\n",
    "data = data.iloc[:, 3:]\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/just_weights.csv'\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"Columns removed and saved to:\", output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed and saved to: /home/wallacelab/complexity-experiment/Final Paper Data/just_hebart49_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the input CSV file\n",
    "input_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/filtered_hebart49_embeddings.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "data = pd.read_csv(input_csv)\n",
    "\n",
    "# Drop the first and last two columns\n",
    "data = data.iloc[:, 1:-2]\n",
    "\n",
    "# Path to the output CSV file\n",
    "output_csv = '/home/wallacelab/complexity-experiment/Final Paper Data/just_hebart49_embeddings.csv'\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "data.to_csv(output_csv, index=False)\n",
    "\n",
    "print(\"Columns removed and saved to:\", output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the new entropy values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "weights_df = pd.read_csv('/home/wallacelab/complexity-experiment/Final Paper Data/just_weights.csv')  \n",
    "matrix_df = pd.read_csv('/home/wallacelab/complexity-experiment/Final Paper Data/just_hebart49_embeddings.csv')  \n",
    "\n",
    "# Filter the weights\n",
    "weights = weights_df['Weight'].values\n",
    "\n",
    "# Convert the entire matrix dataframe to a numpy array\n",
    "matrix = matrix_df.values\n",
    "\n",
    "# Compute the dot products\n",
    "dot_products = np.dot(matrix, weights)\n",
    "\n",
    "# Convert the results into a DataFrame\n",
    "results_df = pd.DataFrame(dot_products, columns=['Dot Product'])\n",
    "\n",
    "# Save the results to a new CSV file\n",
    "results_df.to_csv('/home/wallacelab/complexity-experiment/Final Paper Data/entropy_changes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "complexity_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
